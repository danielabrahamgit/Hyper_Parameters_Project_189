{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for MNIST Digit Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models have some parameters that you can play around with. We saw this last week with ridge regression. We were able to choose the value of $\\lambda$ in order to get our model to behave differently. However one may ask the question, how do we know the best $\\lambda$ for our model? This often depends on many factors and is model dependent. We will be answering this question for a different set of hyper parameters for a different model.\n",
    "\n",
    "In this notebook, we will be using various different tools we have learned in past weeks and explore how to optimize their hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be importing a python module that includes the MNIST handwritten digit dataset for us. In addition we will use skleran's MLPClassifier as our MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data, which is already split into training and testing for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to attempt to visualize data before performing any sort of ML techniques on it. Let us choose a few random indices and display the handwritten digits using the plt.imshow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(train_X[381], cmap = 'gray')\n",
    "plt.figure()\n",
    "imgplot = plt.imshow(train_X[32], cmap = 'gray')\n",
    "plt.figure()\n",
    "imgplot = plt.imshow(train_X[132], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let us take a scenario where we only wish to classify between a handwritten '0' and '1'. So, for this code, we wish to populate the variables $train\\_X\\_bin, train\\_y\\_bin, test\\_X\\_bin, test\\_y_bin$. We want them to be similar to the original testing/training data with the following exception:\n",
    "\n",
    "We want to omit all data points with labels that are not 0 or 1. We also wish to vectorize the images. The structure of the given data is N x 28 x 28, or N (28 x 28) pictures. Instead, we wish to represent each picture, which is just a 2D array of ints, as a flattened 1d array of ints. So, the dimensionality of our new data should be M x $28^2$ and M is definitely smaller than N. \n",
    "\n",
    "### **(TODO) Create a new set of training/testing data that only contains images with labels 0 or 1. Furthermore, flatten the images into 1D arrays of size 28*28**. Store the new test/train split for X and y in the variables defined below. (Hint: numpy's reshape library may come in handy for the image flattening)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_bin, train_y_bin = [], []\n",
    "test_X_bin, test_y_bin = [], []\n",
    "\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Ridge Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try various different machine learning aproaches to classify the handwritten digits. For each of these ML techniques, we will introduce a hyper parameter and have you find the best hyper parameter. Let us start with ridge regression.\n",
    "\n",
    "We have used ridge regression before, but let's remind ourselves how it works. Say we have a data matrix $X \\in \\mathbb{R}^{n \\times d}$. In our case, $n$ is the number of training data for the '0' or '1' handwritten digits. Each image is $28 \\times 28$ pixels, which means that $d = 28^2 = 784$. Our labels will be stored in a vector $\\vec{y} \\in \\mathbb{R}^n$ where $y_i \\in \\{0, 1\\}$. In ridge regression, we solve the following optimization problem: \n",
    "\n",
    "$$\n",
    "\\min_{\\vec{w} \\in \\mathbb{R}^d} ||(X\\vec{w} - \\vec{y})||_2^2 + \\lambda ||\\vec{w}||_2^2, \\; \\; \\lambda > 0\n",
    "$$\n",
    "Where the solution to this optimization problem is $\\vec{w}^* = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$. If we wish to classify a new datapoint $\\vec{x_{new}}$, then we use the folowing scheme: $label_{new} = f(\\vec{x_{new}}^T \\vec{w}^*)$ where $f(x) = 1$ if $x > 0$ and $f(x) = 0$ if $x \\le 0$. Now that we have covered how we will use ridge regression to classify new samples, let us impliment a few helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the function loss_func code. This loss function will simply return the percentage of missclassified datapoints given a test data matrix X_test, the optimal weight w_opt, and the reference labels y_ref (HINT: Use the previous functions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_pred, y_ref):\n",
    "    #TODO\n",
    "    #Start\n",
    "    return None\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the classify function. Upon a scalar input, classify(x) will return the sign of x. On a vector input, classify(x) returns the sign of each element of the vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    #TODO\n",
    "    #Start\n",
    "    return None\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the classify_ridge function. This function will take in the train/test data along with the ridge hyper parameter. It should return the classification test error. You can use Sklearn's ridge model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ridge(X_train, y_train, X_test, y_test, lambd):\n",
    "    #TODO\n",
    "    ridge = Ridge(alpha=lambd, solver='cholesky')\n",
    "    #Start\n",
    "    return None\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a ridge classification function. Now, impliment a hyper-parameter grid search over the powers of 10 to find the most optimal value of lambda. We will provide the range of exponents. If the exponent is 4, then $\\lambda = 10^4$.\n",
    "\n",
    "\n",
    "### **(TODO) Given the EMPTY lists of lambdas and errors, iterate through all the lambdas (powers of exponents given) and populate the lambdas and errors lists. We will use these later for plotting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "errors  = []\n",
    "lambda_vals = np.logspace(-1, 12, 1)\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the different values of lambda with their respective errors.\n",
    "\n",
    "### **(TODO) Plot the values of lambda against the log of the errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Given the plot above, what is the most optimal value of lambda? What is the associated error? Remember that this error is the percentage of miss-classified test points. Would you consider ridge regression to be a good binary classifer?**\n",
    "\n",
    "COMMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Logistic Regression With a Twist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the same sort of classification, except we will use logistic regression. With a twist. Before we get to the twist, let us perform the standard logistic regression. You may use sklearn's logistic regression functionality. Remember, we are classifying on the following variables:\n",
    "\n",
    "$$train\\_X\\_bin, train\\_y\\_bin, test\\_X\\_bin, test\\_y\\_bin$$\n",
    "\n",
    "\n",
    "### **(TODO) Perform logistic regression on the MNIST data. Sklearn's LogisticRegression model may be useful here. Make sure to use  LogisticRegression(penalty = 'none', solver='saga')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "logreg = LogisticRegression(penalty='none', solver='saga')\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Report the training and testing error. You can re-use the loss_func function from before .** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "train_error = None\n",
    "test_error = None\n",
    "#Start\n",
    "#End\n",
    "\n",
    "print('Training Error:', train_error)\n",
    "print('Testing  Error:', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Comment on how well logistic regression performs on binary MNIST digit classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how logistic regression performs so well, we will not try our best to make it perform worse. The way we will do this is to add noise to the training data. We will then see how well it performs at different noise levels.\n",
    "\n",
    "Some background first: We can model noise by simply adding small random numbers to each entry of our matrices. There is much theory behind how to model this noise using random variables, but we will not dive into this. We will provide you with the code that adds noise to the data. The one thing you must understand is that we will be using a parameter, named sigma, that will control the level of noise added. \n",
    "\n",
    "To get a feel for this, let us first plot a simple sine wave without any added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(500)\n",
    "plt.plot(t, np.sin(10 * 2 * np.pi * t / len(t)))\n",
    "plt.title('Simple Sine Wave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have plotted a sine wave, we will add noise to it with this parameter we have mentioned before, sigma. We want you to play around with sigma. Try differnt values of sigma and run the cell below. This will give you a feel for how this parameter changes added noise. Do note that $\\sigma \\ge 0$. You should specifically try values between 0 and 1\n",
    "\n",
    "### **(TODO) Play around with sigma to get a feel for how noise effects the data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different sigma values 0 - 1\n",
    "#Start\n",
    "sigma = 0.2\n",
    "#End\n",
    "\n",
    "noisy_sine = np.sin(10 * 2 * np.pi * t / len(t)) + np.random.normal(0, sigma, len(t))\n",
    "plt.plot(t, noisy_sine)\n",
    "plt.title('Noisy Sine Wave $\\sigma=$' + str(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same thing for our datasets. First, we will create a function that will automatically return a new set of data with noise X samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_data(X_train, X_test, sigma):\n",
    "    X_train_noisy = X_train + np.random.normal(0, sigma, X_train.shape)\n",
    "    X_test_noisy  = X_test  + np.random.normal(0, sigma, X_test.shape)\n",
    "    return X_train_noisy, X_test_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to convert the 1D flattened images back to 2D. This you will impliment:\n",
    "\n",
    "### **(TODO) Impliment the function vec_to_img which takes in a 1d array and the image resolution. It will return a 2d array that represents the image.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_img(vec, res):\n",
    "    #TODO\n",
    "    #Start\n",
    "    return None\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define our new noisy dataset. We will be plotting a few handwritten digits. We encourage you to again play with sigma and see how the images turnout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different sigma values in the range 0-400\n",
    "#Start\n",
    "sigma = 400\n",
    "#End\n",
    "X_train_noisy, X_test_noisy = noisy_data(train_X_bin, test_X_bin, sigma)\n",
    "\n",
    "random_indices = random.sample(range(0, X_train_noisy.shape[0] - 1), 5)\n",
    "for i in random_indices:\n",
    "    imgplot = plt.imshow(vec_to_img(X_train_noisy[i], (28, 28)), cmap = 'gray')\n",
    "    plt.title('Label = ' + str((train_y_bin[i] + 1) // 2))\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how logistic regression performs at different values of sigma. We will provide you a range of sigma values: $\\sigma \\in [0, 100]$. We would like you to populate the list $err\\_lst$. For each value of sigma, find the error and update the list:\n",
    "\n",
    "### **(TODO) Go through all the values of sigma in sigmas. Populate the list err_lst with the error on the test data for a specific value of sigma. Use loss_func to compute the error.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.arange(350,400)\n",
    "err_lst = []\n",
    "logreg = LogisticRegression(penalty='none', solver='saga')\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Now plot the error as a funtion of sigma below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the error start creeping up, as expected. So, we will try to combat this by introducing a hyperparameter for regularization. Just like in ridge regression where we used lambda, logistic regression also has a regularization hyper parameter. We will be using the L2 regularizer to see if we can achieve better results at high error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first show you how to use the regularizer with Sklearn's regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 23.0\n",
    "logreg = LogisticRegression(random_state=0, penalty = 'l2', solver='saga')\n",
    "logreg.set_params(C=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the parameter $c$ is used as the hyper parameter to control the L2 regularization. The larger $c$ is, the smaller the regularization is. That is, $c$ is inversley proportional to the amount of regularization (unlike ridge regression).  \n",
    "\n",
    "So, we will now use $\\sigma = 400$ and try different values of $C$.\n",
    "\n",
    "### **(TODO) Perform hyper parameter optimization via grid search. Use sigma = 400 and try all values of C within the range specified. Store the error in err_lst for each value of C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-15, -1, 10)\n",
    "err_lst = []\n",
    "X_train_noisy, X_test_noisy = noisy_data(train_X_bin, test_X_bin, sigma=400)\n",
    "logreg = LogisticRegression(random_state=0, penalty = 'l2', solver='saga', C=c)\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the error against the different values of C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Which value of C is the best? What is the associated error?.**\n",
    "\n",
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Is this minimum error better than the error without regularization? .**\n",
    "\n",
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Least Squares - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finally use Least Squares in combination with PCAs. Let us mathematically define our model:\n",
    "\n",
    "$$\\min_{\\vec{w} \\in \\mathbb{R}^d} ||X_k\\vec{w} - \\vec{y}||_2^2,\\; \\; X_k = XV_kV_k^T, \\; \\; X = U \\Sigma V^T$$\n",
    "\n",
    "We define $V_k$ to be the first $k$ columns of $V$. In this problem, k is our hyper parameter. Then, to classify, we use:\n",
    "\n",
    "$$\n",
    "    PCA_k(X_{new}) \\vec{w}^* = X_{new}V_{k,new} V_{k,new}^T \\vec{w}^*\n",
    "$$\n",
    "\n",
    "We wish to explore which value of k will yield us the best results.\n",
    "\n",
    "First, we need a least squares function and a PCA funtion. You can use Sklearn's implimentation of each.\n",
    "\n",
    "### **(TODO) Impliment the least_sq_pca_train function. This will perform least squares on the k dimensional representation of the data matrix X. Return the weight vector and the training error. You can use Sklearn's PCA module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_sq_pca_train(X_train, y_train, k):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    return w, train_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the least_sq_pca_test function. This will first perform k-PCA on the test data, then multiplies the k-PCA test data with the optimal vector w. Report the predicted value and the testing error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_sq_pca_test(X_test, y_test, w, k):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    return y_pred, test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the functions to the training and testing data for k = 2:\n",
    "\n",
    "### **(TODO) Using our two helper functions, perform least square-pca classification on the training set to find w, then report the training error. Then find the testing error using the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "train_err = None\n",
    "test_err = None\n",
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "print(str(k) + '-PCA Training Error:', train_err)\n",
    "print(str(k) + '-PCA Testing Error:', test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (hopefully we are seeing a pattern by now) we want to use hyper-parameter grid search in order to find the best k. We have provided the range of Ks for you to use. \n",
    "\n",
    "### **(TODO) Iterate through all values of k in the range in order to find the testing error at each value of k. Store this testing error into the list err_lst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 784, 100)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best values of k happen in the beginning. Let us 'thin the herd' and change the range from (1, 100, 5)\n",
    "\n",
    "### **(TODO) Copy what you have done for the previous part, this time the range is slightly different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 100, 10)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst (Copy from before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we still need to narrow our focus down to everything below 20. Let us change the range yet again:\n",
    "\n",
    "### **(TODO) Copy what you have done for the previous part, this time the range is slightly different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 20, 1)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst (Copy from before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can see where our best performance happens! \n",
    "\n",
    "### **(TODO) Report the best value of k and the corresponding error**\n",
    "\n",
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the technique we used here. We used a grid search, but we did split it into multiple smaller grid searches. We broke it down into k = 1, 101, 201, ... 701 then we did k = 1, 11, 21, ... 91 then finally we did k = 1, 2, 3, ... 20. \n",
    "\n",
    "However, this 'augmented' grid search has it's advantages and disadvantages. Please list the pros and cons of this augmented grid search:\n",
    "\n",
    "### **(TODO) Augmented grid search pros:**\n",
    "\n",
    "COMMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Augmented grid search cons:**\n",
    "\n",
    "COMMENT HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
