{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for MNIST Digit Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models have some parameters that you can play around with. We saw this last week with ridge regression. We were able to choose the value of $\\lambda$ in order to get our model to behave differently. However one may ask the question, how do we know the best $\\lambda$ for our model? This often depends on many factors and is model dependent. We will be answering this question for a different set of hyper parameters for a different model.\n",
    "\n",
    "In this notebook, we will be using various different tools we have learned in past weeks and explore how to optimize their hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be importing a python module that includes the MNIST handwritten digit dataset for us. In addition we will use skleran's MLPClassifier as our MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data, which is already split into training and testing for us. However, we need to introduce a validation set anyways, so let us just append this into one huge data matrix X and label vector y. We will split this up very soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "X = np.vstack((train_X, test_X))\n",
    "y = np.append(train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to attempt to visualize data before performing any sort of ML techniques on it. Let us choose a few random indices and display the handwritten digits using the plt.imshow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(X[381], cmap = 'gray')\n",
    "plt.figure()\n",
    "imgplot = plt.imshow(X[32], cmap = 'gray')\n",
    "plt.figure()\n",
    "imgplot = plt.imshow(X[132], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that each data point is an image of size 28x28. However, we wish to have our data in 1 dimension. So, impliment the following function to flatten the entire data matrix X which has shape N X 28 X 28 to a new data matrix **X_flat** with shape N X $28^2$.\n",
    "\n",
    "### **(TODO) Impliment the flatten_image_matrix function that takes in a data matrix X with input shape (N, 28, 28) to a flattened X with ouput shape (N, 28*28). (HINT: Use np.reshape!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image_matrix(X_image):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    \n",
    "X_flat = flatten_image_matrix(X_image=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let us take a scenario where we only wish to classify between a handwritten '0' and '1'. So, for this code, we wish to populate the variables $train\\_X\\_bin, train\\_y\\_bin, test\\_X\\_bin, test\\_y\\_bin, val\\_X\\_bin, val\\_y\\_bin$. Before we populate these variables, let us augment X_flat and y such that they only contain images with labels 0 or 1:\n",
    "\n",
    "### **(TODO) Impliment the label_selector function. It will take in a list of labels to keep, and return the data matrix X_bin and label vector y_bin with only the desired labels. In addition, make sure that label '0' corresponds to -1 and label '1' coresponds to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_selector(X_all_labels, y_all_labels, labels_to_keep = []):\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    return X_new, y_new\n",
    "\n",
    "\n",
    "X_bin, y_bin = label_selector(X_flat, y, labels_to_keep = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, our last function for data management is to take in our data and split it into a train set, a validation set, and a test set. Here is a breif discription of what each set is used for:\n",
    "\n",
    "**Train Set**: This set is used to train our models on the MNIST data. This usually is a large chunk of the total data. Here, we are allocating 70% of the total data to train out model.\n",
    "\n",
    "**Validation Set**: This set is used to tune our hyper parameters. So, for example, if we use a ridge regression model, we would train the model on the training set, and then tune lambda such that it minimizes the error in the validation set. \n",
    "\n",
    "**Test Set**: This set is used purely to benchmark our model. We do not train nor tune on this set. We only use it to compare out predicted output on the test set.\n",
    "\n",
    "### **(TODO) Your job is to impilment the train_val_test_split function which takens in the data X matrix and y vector and splits it into 3 sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_r = 0.5, val_r = 0.2, test_r = 0.3):\n",
    "    #These are the variables we are returning\n",
    "    train_X, train_y = None, None\n",
    "    val_X, val_y = None, None\n",
    "    test_X, test_y = None, None\n",
    "    \n",
    "    #Define the number of training, val, and testing data points\n",
    "    N = len(y)\n",
    "    N_train = int(N * train_r)\n",
    "    N_val = int(N * val_r)\n",
    "    N_test = int(N * test_r)\n",
    "    \n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y\n",
    "\n",
    "train_X_bin, train_y_bin, val_X_bin, val_y_bin, test_X_bin, test_y_bin = train_val_test_split(X_bin, y_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to classify a new datapoint $\\vec{x_{new}}$, then we use the folowing scheme: $label_{new} = f(\\vec{x_{new}}^T \\vec{w}^*)$ where $\\vec{w}^*$ is a learned parameter and $f(x) = 1$ if $x > 0$ and $f(x) = 0$ if $x \\le 0$.\n",
    "\n",
    "We now wish to define our loss function. Say we have two labels which are in fact just vectors. Since the lables only hold 0s and 1s, our loss function just simply return the amount of differences between the two labels. Or more formally:\n",
    "\n",
    "$$\n",
    "loss\\_func(\\vec{y_{pred}}, \\vec{y_{ref}}) = \\frac{1}{len(\\vec{y_{pred}})}\\sum_{y_{pred, i} \\ne y_{ref, i}}1\n",
    "$$\n",
    "\n",
    "Or, if $y_{ref, i}$ and $y_{pred, i} \\in  {0, 1}$ then:\n",
    "\n",
    "\n",
    "$$\n",
    "loss\\_func(\\vec{y_{pred}}, \\vec{y_{ref}}) = \\frac{1}{len(\\vec{y_{pred}})}\\sum_{i = 1}^{len(\\vec{y_{pred}})} (y_{pred, i} - y_{ref, i})^2 \n",
    "$$\n",
    "\n",
    "\n",
    "### **(TODO) Impliment the function loss_func code. This loss function will simply return the percentage of missclassified datapoints given a test data matrix X_test, the optimal weight w_opt, and the reference labels y_ref (HINT: Use the previous functions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_pred, y_ref):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the classify function. Upon a scalar input, classify(x) will return the sign of x. On a vector input, classify(x) returns the sign of each element of the vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Ridge Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try various different machine learning aproaches to classify the handwritten digits. For each of these ML techniques, we will introduce a hyper parameter and have you find the best hyper parameter. Let us start with ridge regression.\n",
    "\n",
    "We have used ridge regression before, but let's remind ourselves how it works. Say we have a data matrix $X \\in \\mathbb{R}^{n \\times d}$. In our case, $n$ is the number of training data for the '0' or '1' handwritten digits. Each image is $28 \\times 28$ pixels, which means that $d = 28^2 = 784$. Our labels will be stored in a vector $\\vec{y} \\in \\mathbb{R}^n$ where $y_i \\in \\{0, 1\\}$. In ridge regression, we solve the following optimization problem: \n",
    "\n",
    "$$\n",
    "\\min_{\\vec{w} \\in \\mathbb{R}^d} ||(X\\vec{w} - \\vec{y})||_2^2 + \\lambda ||\\vec{w}||_2^2, \\; \\; \\lambda > 0\n",
    "$$\n",
    "Where the solution to this optimization problem is $\\vec{w}^* = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$. If we wish to classify a new datapoint $\\vec{x_{new}}$, then we use the folowing scheme: $label_{new} = f(\\vec{x_{new}}^T \\vec{w}^*)$ where $f(x) = 1$ if $x > 0$ and $f(x) = 0$ if $x \\le 0$. Now that we have covered how we will use ridge regression to classify new samples, let us impliment a few helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the classify_ridge function. This function will take in the train/reference data along with the ridge hyper parameter. It should return the classification test error. You can use Sklearn's ridge model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ridge(X_train, y_train, X_ref, y_ref, lambd):\n",
    "    \"\"\"\n",
    "        X_ref and y_ref represent a data set that we compare the training set to.\n",
    "        That is, we first train our model on X_train and y_train. Then we find a predicted\n",
    "        output by feeding X_ref into our model. Finally, we compare this predcited\n",
    "        output to y_ref. The error between the predicted and the reference will be \n",
    "        our error, which we return.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    ridge = Ridge(alpha=lambd, solver='cholesky')\n",
    "    #Start\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a ridge classification function. Now, impliment a hyper-parameter grid search and random search over the powers of 10 to find the most optimal value of lambda. We will provide the $\\lambda$ values. Make sure to grid search over the validation data. That is, we pass in the validation set for X_ref and y_ref.\n",
    "\n",
    "\n",
    "### **(TODO) Given the EMPTY list err_lst, and err_list_rand iterate through all the lambdas and populate the errors lists. Make sure to populate err_lst, err_list_rand with the validation errors. We will use this list later for plotting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "err_lst  = []\n",
    "lambda_vals = np.logspace(-1, 12, 500)\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "err_lst_rand = []\n",
    "lambda_vals_rand = (10 ** 12 - 10 ** -1) *  np.random.random(500) + 10 ** -1\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(err_lst_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how the validation error varies with the number of iterations for random search and how it varies with the values of lambda for grid search. \n",
    "\n",
    "### **(TODO) Plot the log of the validation errors values against the values of lambda for grid search and against the number of iterations for random search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO PLOT log of errors vs log of lambda_vals \n",
    "#Hint: Use plt.loglog()\n",
    "#Start\n",
    "#End\n",
    "\n",
    "#Display the minimum error and lambda:\n",
    "i_min = np.argmin(err_lst)\n",
    "best_lambda = lambda_vals[i_min]\n",
    "print(\"Best Lambda:\",  format(best_lambda, '10.2E'))\n",
    "print(\"Minimum Validation Error:\",  err_lst[i_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO PLOT log of errors vs log of iterations \n",
    "#Hint: Use plt.loglog()\n",
    "#Start\n",
    "#End\n",
    "\n",
    "#Display the minimum error and lambda:\n",
    "i_min = np.argmin(err_lst_rand)\n",
    "best_lambda = lambda_vals_rand[i_min]\n",
    "print(\"Best Lambda:\",  format(best_lambda, '10.2E'))\n",
    "print(\"Number of Iterations Taken for Best Lambda:\", i_min)\n",
    "print(\"Minimum Validation Error:\",  err_lst_rand[i_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Given the plots above, what are the most optimal values of lambda for each search method? What are the associated validation errors? Remember that this error is the percentage of miss-classified test points.**\n",
    "\n",
    "COMMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have observed, the values that were returned differed between grid search and random search. However, based on the values you returned from random search, you may have seen a better or worse validation error than with grid search. Some things to consider are as follows: If you were able to find a better or similar validation error with random search, consider how many iterations it took to find that value of lambda. Chances are that it was found in significantly less than the 500 iterations we ran both for. We hope this illustrates one of the key considerations when choosing between grid and random search. In some scenarios, it might be worth sacrificing a little bit of accuracy for greatly reducing the the computational complexity. However, you are not guarnteed the optimal value with random search. Especially in lower dimensional problems, grid search will always give you the optimal solution, assuming your grid is fine enough. However, grid search becomes very expensive as the dimensionality of the problem increases, which we will see later on in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Using the lambda found in grid search from the pervious part, report the test error using this value of lambda. Comment on how good ridge regression is for binary classifcation of handwritten digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "print(\"Test Error:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Logistic Regression With a Twist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the same sort of classification, except we will use logistic regression. With a twist. Before we get to the twist, let us perform the standard logistic regression. You may use sklearn's logistic regression functionality. Remember, we are classifying on the following variables:\n",
    "\n",
    "$$train\\_X\\_bin, train\\_y\\_bin, test\\_X\\_bin, test\\_y\\_bin$$\n",
    "\n",
    "\n",
    "### **(TODO) Perform logistic regression on the MNIST data. Sklearn's LogisticRegression model may be useful here. Make sure to use  LogisticRegression(penalty = 'none', solver='saga')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Report the training and testing error. You can re-use the loss_func function from before .** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "print('Training Error:', train_error)\n",
    "print('Testing  Error:', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Comment on how well logistic regression performs on binary MNIST digit classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how logistic regression performs so well, we will not try our best to make it perform worse. The way we will do this is to add noise to the training data. We will then see how well it performs at different noise levels.\n",
    "\n",
    "Some background first: We can model noise by simply adding small random numbers to each entry of our matrices. There is much theory behind how to model this noise using random variables, but we will not dive into this. We will provide you with the code that adds noise to the data. The one thing you must understand is that we will be using a parameter, named sigma, that will control the level of noise added. \n",
    "\n",
    "To get a feel for this, let us first plot a simple sine wave without any added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(500)\n",
    "plt.plot(t, np.sin(10 * 2 * np.pi * t / len(t)))\n",
    "\n",
    "\n",
    "plt.title('Simple Sine Wave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have plotted a sine wave, we will add noise to it with this parameter we have mentioned before, sigma. We want you to play around with sigma. Try differnt values of sigma and run the cell below. This will give you a feel for how this parameter changes added noise. Do note that $\\sigma \\ge 0$. You should specifically try values between 0 and 1\n",
    "\n",
    "### **(TODO) Play around with sigma to get a feel for how noise effects the data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different sigma values 0 - 1\n",
    "#Start\n",
    "#End\n",
    "\n",
    "noisy_sine = np.sin(10 * 2 * np.pi * t / len(t)) + np.random.normal(0, sigma, len(t))\n",
    "plt.plot(t, noisy_sine)\n",
    "plt.title('Noisy Sine Wave $\\sigma=$' + str(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same thing for our datasets. First, we create a function that takes in the train/val/test data images and adds noise to them. The noise paramter is sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_data(X_train, X_val, X_test, sigma):\n",
    "    X_train_noisy = X_train + np.random.normal(0, sigma, X_train.shape)\n",
    "    X_val_noisy = X_val     + np.random.normal(0, sigma, X_val.shape)\n",
    "    X_test_noisy  = X_test  + np.random.normal(0, sigma, X_test.shape)\n",
    "    return X_train_noisy, X_val_noisy, X_test_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to convert the 1D flattened images back to 2D. This you will impliment:\n",
    "\n",
    "### **(TODO) Impliment the function vec_to_img which takes in a 1d array and the image resolution. It will return a 2d array that represents the image.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_img(vec, res):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define our new noisy dataset. We will be plotting a few handwritten digits. We encourage you to again play with sigma and see how the images turnout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try different sigma values in the range 0-400\n",
    "#Start\n",
    "#End\n",
    "\n",
    "#Generate the noisy data\n",
    "X_train_noisy, X_val_noisy, X_test_noisy = noisy_data(train_X_bin, val_X_bin, test_X_bin, sigma)\n",
    "#pick random indices\n",
    "random_indices = random.sample(range(0, X_train_noisy.shape[0] - 1), 5)\n",
    "for i in random_indices:\n",
    "    imgplot = plt.imshow(vec_to_img(X_train_noisy[i], (28, 28)), cmap = 'gray')\n",
    "    plt.title('Label = ' + str((train_y_bin[i] + 1) // 2))\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how logistic regression performs at different values of sigma. We will provide you a range of sigma values: $\\sigma \\in [0, 100]$. We would like you to populate the list $err\\_lst$. For each value of sigma, find the error and update the list:\n",
    "\n",
    "### **(TODO) Go through all the values of sigma in sigmas. Populate the list err_lst with the error on the noisy test data for specific values of sigma. Use loss_func to compute the error.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigmas = np.arange(0, 450, 50)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Now plot the error as a funtion of sigma below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "error_400 = err_lst[-1]\n",
    "print(\"Test Error at sigma = 400:\",  format(error_400 * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the error start creeping up, as expected. So, we will try to combat this by introducing a hyperparameter for regularization. Just like in ridge regression where we used lambda, logistic regression also has a regularization hyper parameter. We will be using the L2 regularizer to see if we can achieve better results at high error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first show you how to use the regularizer with Sklearn's regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 23.0\n",
    "logreg = LogisticRegression(random_state=0, penalty = 'l2', solver='saga')\n",
    "logreg.set_params(C=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the parameter $c$ is used as the hyper parameter to control the L2 regularization. The larger $c$ is, the smaller the regularization is. That is, $c$ is inversley proportional to the amount of regularization (unlike ridge regression).  \n",
    "\n",
    "So, we will now use $\\sigma = 400$ and try different values of $C$. We will then compare the test error with the best value of C (tuned via the validation set) to the test error with no regularization\n",
    "\n",
    "### **(TODO) Perform hyper parameter optimization via grid search. Use sigma = 400 and try all values of C within the range specified. Store the error in err_lst for each value of C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-15, -1, 10)\n",
    "err_lst = []\n",
    "X_train_noisy, X_val_noisy, X_test_noisy = noisy_data(train_X_bin, val_X_bin, test_X_bin, sigma=400)\n",
    "logreg = LogisticRegression(random_state=0, penalty = 'l2', solver='saga')\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the validation error against the different values of C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "#Display the minimum error and lambda:\n",
    "i_min = np.argmin(err_lst)\n",
    "best_C = Cs[i_min]\n",
    "print(\"Best C:\",  format(best_C, '10.2E'))\n",
    "print(\"Minimum Validation Error:\",  format(err_lst[i_min] * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Which value of C is the best? What is the associated error?.**\n",
    "\n",
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to find the test error at this optimal value best_C. \n",
    "\n",
    "### **(TODO) Find the test error at the optimal value best_C. Compare this to the value error_400, the test error with no regularization at sigma = 400.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0, penalty = 'l2', solver='saga')\n",
    "test_error_logreg = None\n",
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "print(\"Test Error With Best Hyperparameter:\", format(test_error_logreg * 100, '10.2f') + '%')\n",
    "print(\"Test Error With No Hyperparameter:\", format(error_400 * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Is this minimum error better than the error without regularization? If so, comment on why you think this is.**\n",
    "\n",
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TEST START**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Least Squares - PCA On Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finally use Least Squares in combination with PCAs. Let us mathematically define our model:\n",
    "\n",
    "Remember that PCA requires de-meaned data. Or, mathematically:\n",
    "\n",
    "$$\n",
    "    \\hat{\\vec{x}} = \\frac{1}{n}\\sum_{i = 1}^n \\vec{x_i}\\\\\n",
    "    \\hat{X} = X - \\hat{\\vec{x}}\n",
    "$$\n",
    "\n",
    "We have slightly abused notation. When we write $X - \\hat{\\vec{x}}$, we are refering to subtracting the row average of $X$ from each row of $X$. This will give us a zero mean version of $X$, which we call $\\hat{X}$. \n",
    "\n",
    "Now our model looks like:\n",
    "\n",
    "$$\\vec{w}^* = \\arg\\min_{\\vec{w} \\in \\mathbb{R}^k} ||X_k\\vec{w} - \\vec{y}||_2^2 \\\\\n",
    "X_k = PCA_k(X) = \\hat{X}V_k, \\; \\; \\hat{X} = U \\Sigma V^T$$\n",
    "\n",
    "We define $V_k$ to be the first $k$ columns of $V$. In this problem, k is our hyper parameter. \n",
    "Then, to classify, we use:\n",
    "\n",
    "$$\n",
    "    PCA_k(X_{new}) \\vec{w}^* = \\hat{X_{new}}V_{k,new} \\vec{w}^*\n",
    "$$\n",
    "\n",
    "We wish to explore which value of k will yield us the best results.\n",
    "\n",
    "First, we need a least squares function and a PCA funtion. You can use Sklearn's implimentation of each.\n",
    "\n",
    "### **(TODO) Impliment the least_sq_pca_train function. This will perform least squares on the k dimensional representation of the data matrix X. Return the weight vector and the training error. (HINT: Use pca.transform followed by pca.inverse_transform)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_sq_pca_train(X_train, y_train, k):\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    return w, train_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Impliment the least_sq_pca_test function. This will first perform k-PCA on the test data, then multiplies the k-PCA test data with the optimal vector w. Report the predicted value and the testing error. (HINT: Use pca.transform followed by pca.inverse_transform)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We use the convention \"ref\" since X_ref and y_ref can represent the \n",
    "    testing or validation data, each used as a reference data set\n",
    "\"\"\"\n",
    "def least_sq_pca_ref(X_ref, y_ref, w, k):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X_ref)\n",
    "    #TODO\n",
    "    #Start\n",
    "    #End\n",
    "    return y_pred, ref_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we evaluate our Least Squares PCA model, let us first just use standard least squares on the noisy dataset. We will use $\\sigma = 400$ again.\n",
    "\n",
    "### **(TODO) Train a standard least squares model on the noisy training set, and report the error on the noisy testing set and store the weight vector in w_OLS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noisy, X_val_noisy, X_test_noisy = noisy_data(train_X_bin, val_X_bin, test_X_bin, sigma=400)\n",
    "w_OLS = None\n",
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "print('Ordinary Least Squares Testing Error:', format(test_err_OLS * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the OLS testing error on the noisy data-set is roughly 15%. Let us see if we can do better with our K-PCA-OLS functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (hopefully we are seeing a pattern by now) we want to use hyper-parameter grid search in order to find the best k. We have provided the range of Ks for you to use. \n",
    "\n",
    "### **(TODO) Iterate through all values of k in the range in order to fit the model on the noisy training data. Then, find the validation error error at each value of k. Store this validation error into the list err_lst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 784, 100)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best values of k happen in the beginning. Let us 'thin the herd' and change the range from (1, 200, 10)\n",
    "\n",
    "### **(TODO) Copy what you have done for the previous part, this time the range is slightly different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 200, 10)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst (Copy from before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we still need to narrow our focus down to below 100. Let us change the range yet again. This one might take some time:\n",
    "\n",
    "### **(TODO) Copy what you have done for the previous part, this time the range is slightly different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 100, 2)\n",
    "err_lst = []\n",
    "#TODO\n",
    "#Start\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Plot the values of k vs the err_lst (Copy from before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "\n",
    "#Display the minimum error and lambda:\n",
    "i_min = np.argmin(err_lst)\n",
    "best_k = ks[i_min]\n",
    "print(\"Best K:\",  format(best_k, '10.2E'))\n",
    "print(\"Minimum Validation Error:\",  format(err_lst[i_min] * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can see where our best performance happens! \n",
    "\n",
    "### **(TODO) Using best_k, find the noisy testing error and w_k (the weight vector). Is the error better than the standard least squares noisy testing error?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err_kpca = None\n",
    "w_k = None\n",
    "#TODO\n",
    "#Start\n",
    "#End\n",
    "\n",
    "\n",
    "print(\"Optimal K-PCA OLS Test Error:    \", format(test_err_kpca * 100, '10.2f') + '%')\n",
    "print(\"Standard OLS (no PCA) Test Error:\", format(test_err_OLS * 100, '10.2f') + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize the weight learned by K-PCA OLS and the weight learned by Standard OLS. We will plot the weights as an image such that the brighter pixels correspons to higher weights for that pixel. That is, if the weights (as an image) is completely dark, then our weight vector must be close to zero. We will provide the visualization function for you. You will need to interpret what this each plot means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(w, class_type = ''):\n",
    "    plt.imshow(vec_to_img(w, (28, 28)) ** 2, cmap = 'gray')\n",
    "    plt.title(class_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(w_k, str(best_k) + '-PCA Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(w_OLS, 'Sandard OLS Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Interpret the images above. What does this tell us about the optimal weight vector? Which pixels does it prioritize?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the technique we used here. We used a grid search, but we did split it into multiple smaller grid searches. We broke it down into k = 1, 101, 201, ... 701 then we did k = 5, 10, 15, ... 200 then finally we did k = 1, 3, 5, ... 100\n",
    "\n",
    "However, this 'augmented' grid search has it's advantages and disadvantages. Please list the pros and cons of this augmented grid search:\n",
    "\n",
    "### **(TODO) Augmented grid search pros:**\n",
    "\n",
    "COMMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Augmented grid search cons:**\n",
    "\n",
    "COMMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an MLP Classifier on the Noisy Data\n",
    "\n",
    "So far you have experimented with grid search on the hyper parameters associated with ridge regression, logistic regression and PCA. Now, let us try the same thing on a multi layer perceptron classifier, a type of neural net. We will keep things somewhat simple by ommitting some of the parameters from the search since they will be covered more in-depth in the neural networks module. It is recommended that you are familiar with MLPs and to read through the neural networks section of the hyper paramter notes before beginning this part. We will be using sklearn's MLP classifer function along with its GridSearchCV functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will begin by running the MLP classifer with its default parameters for 300 iteration to see how it performs. \n",
    "If you are curious you can check to see what the default parameters are on the sklearn.neural_netwrk.MLPClassifer \n",
    "documentation page (you will likely have to reference this page regardless to complete some of the code).\n",
    "Again, you will not know what all of the parameters listed in the documentation do, some will be introduced \n",
    "in the neural nets module and others will be introduced in future, advanced coursework. \n",
    "\"\"\"\n",
    "#Generate data\n",
    "X_train_noisy, X_val_noisy, X_test_noisy = noisy_data(train_X_bin, val_X_bin, test_X_bin, sigma=600)\n",
    "MLP = MLPClassifier(random_state = 1, max_iter = 100)   #Initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(TODO) Implement Sklearn grid search on the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will use the GridSearchCV functions of Sklearn to try and find an optimal set of hyperparameters. We have already included the possible parameter values/ranges, you will simply have to feed them in. Since we assume you don't want to have to sit in your computer waiting for it to train, we've only included a few hyperparameters to test out. However, once you finish the assignment you should see that it is very easy to generalize to the other hyperparameters as well and you are encourgaed to experiment with different values/ranges/hyperparameters. Since you have already implemented grid search manually earlier with for loops, we will be using Sklearn here to gain exposure to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "hidden_layer_sizes = [500]\n",
    "activation_fxns = ['relu', 'tanh']\n",
    "solvers = ['sgd', 'adam']\n",
    "learning_rates = [0.001, 0.01]\n",
    "#START\n",
    "#Implement GridSearchCV \n",
    "#END\n",
    "#This took about 10 minutes to run, your time may vary depending on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the optimal hyperparameters as well as the error on the test set\n",
    "#TODO\n",
    "#START\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall earlier when we did comapred grid search and random search for ridge regression. It may have seemed trivial then since only one hyper parameter was being optimized, but hopefully you see now the potential issues with grid search. Even though we were running a very small problem with 8 combination of hyper parameters and only 100 iterations per combination, it took around 10 minutes. This is still very much considered a low dimensional problem. One can infer how computationally heavy it would become with much more data, features and combinations. In those cases, it may be more prudent to do a random search or an adaptive random search as described in the notes, in order to still get a good result using less computation. Hyperparameter optimization is still an area of ongoing research and interested students may look up research papers or look into other techniques such as Bayseian optimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
